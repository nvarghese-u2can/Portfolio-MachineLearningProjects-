# -*- coding: utf-8 -*-
"""NamedEntityRecogniztion_Model_with_Test_Application.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DhILk_XoMYopgdB8ewef6L_V8A_BWD-h

Author : Nelson Vithayathil Varghese

This is the Natural Language Processing (NLP) model implementation file for the Named Entity Recogniztion (NER) Task .

I would be adding the comments at each step to reflect the entire thought process that I have followed during the develpment of this model

# **PART-1 :: Designing of the NER Model**

**Brief note on Named-entity recognition (NER)**

Named-entity recognition (NER) (also known in another names such as named entity identification,token classification or entity extraction) can be treated as a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. From a Machine Learning's perspective, I view  it as supersvised learning,multi-class classfication problem where each class denotes the various entities that are mentioned above.Here the data source is an unstructured text

**NER NLP Model Design Approach**

In order to develop this model, I would be adopting a Deep Learning oriented approach wherein  the state-of-the-art method of Transfer Learning with Transformers would be employed.As per my understanding, this method has two key steps: pre-training followed by fine-tuning. 

Firstly use a pre-trained language model  such as (Transformer stack namely BERT)  that was trained on the large corpus of textual data (from a source like wikipedia),then subsequently training this model on the custom dataset used for this specific problem( Kaggle Dataset named Annotated Corpus for Named Entity Recognition(Datasetfile name: ner_dataset.csv) ) and specialize / tune it for the given NER task.

As stated above, I would be using pre-trained Transformer encoder( BERT) model from the Hugging face

simpletransformers is a python package library that is based on the Transformers library by the Hugging Face and  basically it is built as a wrapper around the t Transformers library made by Hugging Face.

Reference websites that I have used while building the model.

https://pypi.org/project/simpletransformers/

https://simpletransformers.ai/docs/ner-model/

https://huggingface.co/models?pipeline_tag=token-classification&sort=downloads

https://www.kaggle.com/datasets/abhinavwalia95/entity-annotated-corpus
"""

!pip install simpletransformers

import os
import numpy as np
import pandas as pd
import seaborn as sns

"""# Obtain the data

I am using a helper function that can be used from both colab / Jupyter notebook environment to fetch the data into a Pandas dataframe for further analysis
"""

# Helper funtion for reading the csv dataset file and generating the dataframe
def load_dataset(file_name):
    
    data_path = os.path.join(os.getcwd(),file_name)
#     return pd.read_excel(data_path,index_col = 0)
    return pd.read_csv(data_path,encoding='latin1')

df_data = load_dataset('ner_dataset.csv')
df_data.head(27)

"""# Exploratory Data Analysis(EDA)"""

df_data.info()

df_data.columns

df_data['Tag'].nunique()

# Named Entity Values ( Label / Ground Truth Values)
tags = df_data['Tag'].unique()
pd.DataFrame(data=tags,columns=['Named Entities[Label Values]'])

# Similar kind of analysis for the POS ( Parts Of Speach column)
tags = df_data['POS'].unique()
pd.DataFrame(data=tags,columns=['Parts Of Speach Values'])

# Visually check for any missing / null values
sns.heatmap(df_data.isnull(),xticklabels=False,yticklabels=False,cbar = False,cmap='viridis')

"""# Preprocessing"""

# Sentence column has NaN values for the words other than teh first word of a sentence in Sentence #
df_data.loc[:,'Sentence #'] = df_data['Sentence #'].fillna(method='ffill')

df_data.head(27)

# Visually check for any missing / null values
sns.heatmap(df_data.isnull(),xticklabels=False,yticklabels=False,cbar = False,cmap='viridis')

df_data['Sentence #'].unique()

# Sentence column names ( Ex. Sentence 1) for each statement needs to be converted a numerical column 
# before building the model
from sklearn.preprocessing import LabelEncoder

df_data['Sentence #'] = LabelEncoder().fit_transform(df_data['Sentence #'] )

df_model_data = df_data.drop('POS', axis=1)

# For better readability 
df_model_data.rename(columns={'Sentence #':'sentence_id','Word':'words','Tag':'labels'}, inplace =True)

df_model_data.columns

X = df_model_data.drop('labels',axis=1)

y = df_model_data['labels']

"""# Model Building"""

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)

# model_train_data frame
model_train_data = pd.DataFrame({'sentence_id':X_train['sentence_id'],'words':X_train['words'],'labels':y_train})

# model_test_data frame
model_test_data = pd.DataFrame({'sentence_id':X_test['sentence_id'],'words':X_test['words'],'labels':y_test})

from simpletransformers.ner import NERModel,NERArgs

# Get the list of all the unique NER Tags from the dataset
labels = df_data['Tag'].unique().tolist()
# Changing it to uppercase for better readability
# labels = [label.upper() for label in labels]
# labels

args = NERArgs()
args.num_train_epochs = 4
args.learning_rate = 1e-4
args.overwrite_output_dir =True
args.train_batch_size = 32
args.eval_batch_size = 32

import torch
cuda_available = torch.cuda.is_available()
cuda_available

model = NERModel('bert', 'bert-base-cased',labels=labels,args=args,use_cuda=cuda_available)
# model = NERModel('bert', 'bert-large-cased',labels=labels,args=args,use_cuda=cuda_available)
# model = NERModel('bert', 'dslim/bert-base-NER',labels=labels,args=args,use_cuda=cuda_available)

model.train_model(model_train_data,eval_data = model_test_data)

result, model_outputs, wrong_preds = model.eval_model(model_test_data)

result

"""# **PART-2 :: Application based on  the NER Model**

Part-2 has following major sub-components

1) Unit Test program to test the NER Model 

2) Web Scraper Application for the Google News Feed 

3) A database to store the execution results of NER Model testing for the text data generated from step-2

# Unit Test Program for the NER Model
"""

# This is the unit test code for the NER Model validation
def unit_test_NER_model(test_data):
  prediction, model_output = model.predict(test_data)
  print(prediction)
  print('\n\n')

if __name__ == '__main__':
  # Set test_data to something other than 'quit'.
  test_data = ''
  # Start a loop that will run until the user enters 'quit'.
  while test_data != 'quit':
    # Ask the user for a input.
    test_data = input("Please enter the unit test input for the  NER Model, or enter 'quit': ")    
    
    if test_data != 'quit':
      unit_test_NER_model([test_data])
    else:
      # Quiting the test .
      print('Quiting the Unit Test for NER Model')

"""# Web Scraper Application for the Google News Feed

pygooglenews - A python wrapper of the Google News RSS feed

 https://pypi.org/project/pygooglenews/
"""

!pip install pygooglenews

from pygooglenews import GoogleNews

gnews = GoogleNews(country='Canada')

def get_news(search_title):
  stories = []
  search = gnews.search(search_title)
  newsitem = search['entries']
  for item in newsitem:
    story = [item.title]            
    stories.append(story)
    
  return stories
  
if __name__ == '__main__':
  news_data = get_news('Ukraine')
  print(news_data)
  print('\n\n')
  for item in range(len(news_data)):
    prediction, model_output = model.predict(news_data[item])
    print(prediction)

"""# Adding the Database Support to Store the  Execution Results of NER Model Testing"""

import sqlite3
from pygooglenews import GoogleNews

gnews = GoogleNews(country='Canada')

def get_news(search_title):
  stories = []
  search = gnews.search(search_title)
  newsitem = search['entries']
  for item in newsitem:
    story = [item.title]            
    stories.append(story)
    
  return stories
  
if __name__ == '__main__':
  news_data = get_news('Ukraine')
  # conn = sqlite3.connect('ner.db')
  # Creating the database within the memoryitself  
  conn = sqlite3.connect(':memory:')
  cur = conn.cursor()
  try:

    cur.execute('''CREATE TABLE NERTESTRESULTS
           (NER LIST OF NER DICTIONARY
           )''')
    
    print ("Table - NERTESTRESULTS created successfully")
  except:
    pass

  for item in range(len(news_data)):
    prediction, model_output = model.predict(news_data[item])
    # print(prediction[0])

    for item in range(len(prediction[0])):
      data = prediction[0][item]

      cur.execute("INSERT INTO NERTESTRESULTS (NER)  VALUES (?)", [str(data)])
      conn.commit()

    latest_table_contents = cur.execute("""SELECT * FROM NERTESTRESULTS""")
    print(latest_table_contents.fetchall())
    
  conn.close()

